{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2186e06e-bbc8-4b8f-836e-c425f43cd307",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/07/10 16:56:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- reviews_count: integer (nullable = true)\n",
      " |-- postal_code: integer (nullable = true)\n",
      " |-- general_info: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- locality: string (nullable = true)\n",
      " |-- breadcrumbs: string (nullable = true)\n",
      " |-- average_rating: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- website: string (nullable = true)\n",
      " |-- years_in_business: double (nullable = true)\n",
      " |-- full_address: string (nullable = true)\n",
      " |-- phone: string (nullable = true)\n",
      " |-- payment_mode: string (nullable = true)\n",
      " |-- price_range: string (nullable = true)\n",
      " |-- geo: string (nullable = true)\n",
      " |-- gallery: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- scraped_at: string (nullable = true)\n",
      " |-- opening_hours: string (nullable = true)\n",
      " |-- aka: string (nullable = true)\n",
      " |-- street_address: string (nullable = true)\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- County: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Zip code: integer (nullable = true)\n",
      " |-- Agency responsible: string (nullable = true)\n",
      " |-- Cause of death: string (nullable = true)\n",
      " |-- Circumstances surrounding death: string (nullable = true)\n",
      " |-- Symptoms of mental illness: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/10 16:56:47 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+--------------------+--------------------+------------+--------------------+--------------+--------+--------------------+--------------------+-----------------+--------------------+--------------+--------------------+-----------+--------------------+--------------------+--------------------+--------------------+-------+------+---------+-------------------+--------------------+--------------------+--------------------+----+----+------+----+----+----+------+-----+--------+------------------+--------------+-------------------------------+--------------------------+\n",
      "|reviews_count|postal_code|        general_info|            category|    locality|         breadcrumbs|average_rating|latitude|               email|             website|years_in_business|        full_address|         phone|        payment_mode|price_range|                 geo|             gallery|                name|                 url|country|region|longitude|         scraped_at|       opening_hours|                 aka|      street_address|  ID| Age|Gender|Race|Date|City|County|State|Zip code|Agency responsible|Cause of death|Circumstances surrounding death|Symptoms of mental illness|\n",
      "+-------------+-----------+--------------------+--------------------+------------+--------------------+--------------+--------+--------------------+--------------------+-----------------+--------------------+--------------+--------------------+-----------+--------------------+--------------------+--------------------+--------------------+-------+------+---------+-------------------+--------------------+--------------------+--------------------+----+----+------+----+----+----+------+-----+--------+------------------+--------------+-------------------------------+--------------------------+\n",
      "|            3|      30303|                    |Health & Wellness...|     Atlanta|Home/GA/Atlanta/H...|           4.5|33.76014|life@wellnessplus...|http://www.wellne...|             NULL|PostalAddress, US...|(404) 254-1187|amex, carte blanc...|           |latitude: 33.7601...|                    |Wellness Plus Clinic|https://www.yello...|     US|    GA|-84.38643|2021-11-20 03:09:45|   Mo-Fr 10:00-17:00|                    |229 Peachtree St ...|NULL|NULL|  NULL|NULL|NULL|NULL|  NULL| NULL|    NULL|              NULL|          NULL|                           NULL|                      NULL|\n",
      "|         NULL|      11201|Heights Aesthetic...|Hair Removal/Beau...|    Brooklyn|Home/NY/Brooklyn/...|          NULL|40.69469|info@heightsaesth...|http://www.height...|             10.0|PostalAddress, US...|(347) 246-5760|   master card, visa|           |latitude: 40.6946...|                    |Heights Aesthetic...|https://www.yello...|     US|    NY|-73.99344|2021-11-20 03:10:17|   Mo-Sa 10:00-19:00|                    |     147 Montague St|NULL|NULL|  NULL|NULL|NULL|NULL|  NULL| NULL|    NULL|              NULL|          NULL|                           NULL|                      NULL|\n",
      "|         NULL|      11209|                    |Skin Care/Beauty ...|    Brooklyn|Home/NY/Brooklyn/...|          NULL|40.63264| rinnovany@gmail.com|http://www.rinnov...|              7.0|PostalAddress, US...|(347) 979-9262|all major credit ...|           |latitude: 40.6326...|https://i3.ypcdn....|    RINNOVA Medi Spa|https://www.yello...|     US|    NY|-74.03338|2021-11-20 03:10:18|Mo-Tu 11:00-19:00...|                    |    7502 Colonial Rd|NULL|NULL|  NULL|NULL|NULL|NULL|  NULL| NULL|    NULL|              NULL|          NULL|                           NULL|                      NULL|\n",
      "|         NULL|      44136|                    |Optical Goods/Con...|Strongsville|Home/OH/Strongsvi...|          NULL|41.31095|strongsville@patc...|http://www.searso...|             58.0|PostalAddress, US...|(440) 846-3640|master card, visa...|           |latitude: 41.3109...|https://i1.ypcdn....|       Sears Optical|https://www.yello...|     US|    OH|-81.82129|2021-11-20 03:10:23|                    |Sears Holdings Co...| 17271 Southpark Ctr|NULL|NULL|  NULL|NULL|NULL|NULL|  NULL| NULL|    NULL|              NULL|          NULL|                           NULL|                      NULL|\n",
      "|            1|      70118|                    |Colleges & Univer...| New Orleans|Home/LA/New Orlea...|           5.0|29.93661|  apbanos@tulane.edu|   http://tulane.edu|             NULL|PostalAddress, US...|(504) 865-5000|                    |           |latitude: 29.9366...|                    |   Tulane University|https://www.yello...|     US|    LA|-90.12199|2021-11-20 03:10:55|   Mo-Fr 08:30-17:00|                    |6823 Saint Charle...|NULL|NULL|  NULL|NULL|NULL|NULL|  NULL| NULL|    NULL|              NULL|          NULL|                           NULL|                      NULL|\n",
      "+-------------+-----------+--------------------+--------------------+------------+--------------------+--------------+--------+--------------------+--------------------+-----------------+--------------------+--------------+--------------------+-----------+--------------------+--------------------+--------------------+--------------------+-------+------+---------+-------------------+--------------------+--------------------+--------------------+----+----+------+----+----+----+------+-----+--------+------------------+--------------+-------------------------------+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['SPARK_HOME'] = '/usr/local/spark'\n",
    "\n",
    "#!pip install findspark\n",
    "import findspark \n",
    "#!pip install pyspark==2.4.4 \n",
    "\n",
    "findspark.init(\"/usr/local/spark\") \n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Path to the MySQL Connector/J JAR file\n",
    "mysql_connector_jar = \"/usr/local/spark/jars/mysql-connector-java-8.0.30.jar\"\n",
    "\n",
    "# Initialize Spark session with the MySQL Connector/J and authentication settings\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .appName(\"MariaDBIntegration\") \\\n",
    "    .config(\"spark.jars\", mysql_connector_jar) \\\n",
    "    .config(\"spark.authenticate\", \"true\") \\\n",
    "    .config(\"spark.authenticate.secret\", \"5c4d9df1d6970e4fb81979fd70d0ddfb\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# JDBC URL for MariaDB\n",
    "jdbc_url = \"jdbc:mysql://127.0.0.1:3306/medical_services_data\"\n",
    "\n",
    "# Table name in MariaDB\n",
    "table_name = \"medical_services\"\n",
    "\n",
    "# Database properties\n",
    "db_properties = {\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"AzurePassword\",  # Replace with your actual password\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "}\n",
    "\n",
    "# Load data from MariaDB\n",
    "df_mariadb = spark.read.jdbc(jdbc_url, table_name, properties=db_properties)\n",
    "\n",
    "# Show the schema and some sample data\n",
    "df_mariadb.printSchema()\n",
    "df_mariadb.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04d99ea0-d9bc-4ea8-92fb-a54906c16f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/10 17:17:16 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- my_table.health_center_site_fact_identification_number: integer (nullable = true)\n",
      " |-- my_table.site_name: string (nullable = true)\n",
      " |-- my_table.uds_number: string (nullable = true)\n",
      " |-- my_table.site_telephone_number: string (nullable = true)\n",
      " |-- my_table.site_facsimile_telephone_number: string (nullable = true)\n",
      " |-- my_table.administrative_telephone_number: string (nullable = true)\n",
      " |-- my_table.site_web_address: string (nullable = true)\n",
      " |-- my_table.operating_hours_per_week: double (nullable = true)\n",
      " |-- my_table.health_center_grantee_identification_number: double (nullable = true)\n",
      " |-- my_table.grant_number: string (nullable = true)\n",
      " |-- my_table.grant_telephone_number: double (nullable = true)\n",
      " |-- my_table.grantee_name: string (nullable = true)\n",
      " |-- my_table.grantee_uniform_resource_locator_url_: double (nullable = true)\n",
      " |-- my_table.grantee_organization_type_description: string (nullable = true)\n",
      " |-- my_table.migrant_health_centers_hrsa_grant_subprogram_indicator: string (nullable = true)\n",
      " |-- my_table.community_health_hrsa_grant_subprogram_indicator: string (nullable = true)\n",
      " |-- my_table.school_based_health_center_hrsa_grant_subprogram_indicator: string (nullable = true)\n",
      " |-- my_table.public_housing_hrsa_grant_subprogram_indicator: string (nullable = true)\n",
      " |-- my_table.health_care_for_the_homeless_hrsa_grant_subprogram_indicator: string (nullable = true)\n",
      " |-- my_table.site_address_identification_number: integer (nullable = true)\n",
      " |-- my_table.site_address_source_system_identification_number: string (nullable = true)\n",
      " |-- my_table.enterprise_site_repository_source_id: string (nullable = true)\n",
      " |-- my_table.site_data_source: string (nullable = true)\n",
      " |-- my_table.site_address: string (nullable = true)\n",
      " |-- my_table.site_city: string (nullable = true)\n",
      " |-- my_table.site_state_abbreviation: string (nullable = true)\n",
      " |-- my_table.site_postal_code: string (nullable = true)\n",
      " |-- my_table.u_s_mexico_border_100_kilometer_indicator: string (nullable = true)\n",
      " |-- my_table.health_center_site_population_type_description: string (nullable = true)\n",
      " |-- my_table.health_center_location_setting_identification_number: integer (nullable = true)\n",
      " |-- my_table.health_center_service_delivery_site_location_setting_description: string (nullable = true)\n",
      " |-- my_table.health_center_status_identification_number: integer (nullable = true)\n",
      " |-- my_table.site_status_description: string (nullable = true)\n",
      " |-- my_table.health_center_location_identification_number: integer (nullable = true)\n",
      " |-- my_table.health_center_location_type_description: string (nullable = true)\n",
      " |-- my_table.health_center_type_identification_number: integer (nullable = true)\n",
      " |-- my_table.health_center_type_description: string (nullable = true)\n",
      " |-- my_table.health_center_operator_identification_number: integer (nullable = true)\n",
      " |-- my_table.health_center_operator_description: string (nullable = true)\n",
      " |-- my_table.health_center_operating_schedule_identification_number: integer (nullable = true)\n",
      " |-- my_table.health_center_operational_schedule_description: string (nullable = true)\n",
      " |-- my_table.health_center_operating_schedule_identification_number_1: integer (nullable = true)\n",
      " |-- my_table.health_center_operating_calendar: string (nullable = true)\n",
      " |-- my_table.county_fips: integer (nullable = true)\n",
      " |-- my_table.county_equivalent_name: string (nullable = true)\n",
      " |-- my_table.county_description: string (nullable = true)\n",
      " |-- my_table.complete_county_name: string (nullable = true)\n",
      " |-- my_table.state_county_fips_code: integer (nullable = true)\n",
      " |-- my_table.state_fips_code: integer (nullable = true)\n",
      " |-- my_table.state_name: string (nullable = true)\n",
      " |-- my_table.name_of_u_s_senator_number_one: string (nullable = true)\n",
      " |-- my_table.name_of_u_s_senator_number_two: string (nullable = true)\n",
      " |-- my_table.hhs_region_code: integer (nullable = true)\n",
      " |-- my_table.hhs_region_name: string (nullable = true)\n",
      " |-- my_table.u_s_mexico_border_county_indicator: string (nullable = true)\n",
      " |-- my_table.congressional_district_number: integer (nullable = true)\n",
      " |-- my_table.congressional_district_name: string (nullable = true)\n",
      " |-- my_table.state_fips_and_congressional_district_number_code: integer (nullable = true)\n",
      " |-- my_table.u_s_congressional_representative_name: string (nullable = true)\n",
      " |-- my_table.uniform_resource_locator_url_for_a_member_of_the_u_s_house_of_representatives: string (nullable = true)\n",
      " |-- my_table.data_warehouse_record_create_date_text: string (nullable = true)\n",
      " |-- my_table.data_warehouse_record_create_date: string (nullable = true)\n",
      " |-- my_table.termination_date: string (nullable = true)\n",
      " |-- my_table.bphc_assigned_number: string (nullable = true)\n",
      " |-- my_table.site_open_date: string (nullable = true)\n",
      " |-- my_table.site_open_date_text: string (nullable = true)\n",
      " |-- my_table.site_added_to_scope_this_date: string (nullable = true)\n",
      " |-- my_table.site_add_to_scope_date_text: string (nullable = true)\n",
      " |-- my_table.mailing_address: string (nullable = true)\n",
      " |-- my_table.mailing_city: string (nullable = true)\n",
      " |-- my_table.mailing_state_abbreviation: string (nullable = true)\n",
      " |-- my_table.mailing_zip_code: string (nullable = true)\n",
      " |-- my_table.bhcmis_site_identification_number: string (nullable = true)\n",
      " |-- my_table.federally_qualified_health_center_fqhc_look_alike_number: string (nullable = true)\n",
      " |-- my_table.federally_qualified_health_center_fqhc_look_alike_organization_name: string (nullable = true)\n",
      " |-- my_table.federally_qualified_health_center_fqhc_look_alike_street_address: string (nullable = true)\n",
      " |-- my_table.federally_qualified_health_center_fqhc_look_alike_organization_city: string (nullable = true)\n",
      " |-- my_table.federally_qualified_health_center_fqhc_look_alike_organization_state: string (nullable = true)\n",
      " |-- my_table.federally_qualified_health_center_fqhc_look_alike_organization_zip_code: string (nullable = true)\n",
      " |-- my_table.federally_qualified_health_center_fqhc_look_alike_organization_site_administrator_contact_name: string (nullable = true)\n",
      " |-- my_table.federally_qualified_health_center_fqhc_look_alike_organization_site_administrator_contact_telephone_number: string (nullable = true)\n",
      " |-- my_table.federally_qualified_health_center_fqhc_look_alike_organization_site_administrator_contact_email_address: string (nullable = true)\n",
      " |-- my_table.geocoding_artifact_address_primary_x_coordinate: double (nullable = true)\n",
      " |-- my_table.geocoding_artifact_address_primary_y_coordinate: double (nullable = true)\n",
      " |-- my_table.unnamed_84: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/10 17:17:19 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 1)/ 1]\n",
      "java.sql.SQLException: Cannot convert column 1 to integerjava.lang.NumberFormatException: For input string: \"my_table.health_center_site_fact_identification_number\"\n",
      "\tat org.apache.hive.jdbc.HiveBaseResultSet.getInt(HiveBaseResultSet.java:352)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$makeGetter$7(JdbcUtils.scala:429)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$makeGetter$7$adapted(JdbcUtils.scala:428)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anon$1.getNext(JdbcUtils.scala:358)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anon$1.getNext(JdbcUtils.scala:340)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.NumberFormatException: For input string: \"my_table.health_center_site_fact_identification_number\"\n",
      "\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n",
      "\tat java.lang.Integer.parseInt(Integer.java:580)\n",
      "\tat java.lang.Integer.parseInt(Integer.java:615)\n",
      "\tat org.apache.hive.jdbc.HiveBaseResultSet.getInt(HiveBaseResultSet.java:347)\n",
      "\t... 27 more\n",
      "24/07/10 17:17:19 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 1) (myvm.internal.cloudapp.net executor driver): java.sql.SQLException: Cannot convert column 1 to integerjava.lang.NumberFormatException: For input string: \"my_table.health_center_site_fact_identification_number\"\n",
      "\tat org.apache.hive.jdbc.HiveBaseResultSet.getInt(HiveBaseResultSet.java:352)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$makeGetter$7(JdbcUtils.scala:429)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$makeGetter$7$adapted(JdbcUtils.scala:428)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anon$1.getNext(JdbcUtils.scala:358)\n",
      "\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anon$1.getNext(JdbcUtils.scala:340)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.NumberFormatException: For input string: \"my_table.health_center_site_fact_identification_number\"\n",
      "\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n",
      "\tat java.lang.Integer.parseInt(Integer.java:580)\n",
      "\tat java.lang.Integer.parseInt(Integer.java:615)\n",
      "\tat org.apache.hive.jdbc.HiveBaseResultSet.getInt(HiveBaseResultSet.java:347)\n",
      "\t... 27 more\n",
      "\n",
      "24/07/10 17:17:19 ERROR TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o38.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (myvm.internal.cloudapp.net executor driver): java.sql.SQLException: Cannot convert column 1 to integerjava.lang.NumberFormatException: For input string: \"my_table.health_center_site_fact_identification_number\"\n\tat org.apache.hive.jdbc.HiveBaseResultSet.getInt(HiveBaseResultSet.java:352)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$makeGetter$7(JdbcUtils.scala:429)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$makeGetter$7$adapted(JdbcUtils.scala:428)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anon$1.getNext(JdbcUtils.scala:358)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anon$1.getNext(JdbcUtils.scala:340)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.NumberFormatException: For input string: \"my_table.health_center_site_fact_identification_number\"\n\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n\tat java.lang.Integer.parseInt(Integer.java:580)\n\tat java.lang.Integer.parseInt(Integer.java:615)\n\tat org.apache.hive.jdbc.HiveBaseResultSet.getInt(HiveBaseResultSet.java:347)\n\t... 27 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.sql.SQLException: Cannot convert column 1 to integerjava.lang.NumberFormatException: For input string: \"my_table.health_center_site_fact_identification_number\"\n\tat org.apache.hive.jdbc.HiveBaseResultSet.getInt(HiveBaseResultSet.java:352)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$makeGetter$7(JdbcUtils.scala:429)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$makeGetter$7$adapted(JdbcUtils.scala:428)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anon$1.getNext(JdbcUtils.scala:358)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anon$1.getNext(JdbcUtils.scala:340)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.NumberFormatException: For input string: \"my_table.health_center_site_fact_identification_number\"\n\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n\tat java.lang.Integer.parseInt(Integer.java:580)\n\tat java.lang.Integer.parseInt(Integer.java:615)\n\tat org.apache.hive.jdbc.HiveBaseResultSet.getInt(HiveBaseResultSet.java:347)\n\t... 27 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Show the schema and some sample data\u001b[39;00m\n\u001b[1;32m     30\u001b[0m df_hive\u001b[38;5;241m.\u001b[39mprintSchema()\n\u001b[0;32m---> 31\u001b[0m \u001b[43mdf_hive\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:945\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    886\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \n\u001b[1;32m    888\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 945\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:963\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    958\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    959\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    960\u001b[0m     )\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o38.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (myvm.internal.cloudapp.net executor driver): java.sql.SQLException: Cannot convert column 1 to integerjava.lang.NumberFormatException: For input string: \"my_table.health_center_site_fact_identification_number\"\n\tat org.apache.hive.jdbc.HiveBaseResultSet.getInt(HiveBaseResultSet.java:352)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$makeGetter$7(JdbcUtils.scala:429)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$makeGetter$7$adapted(JdbcUtils.scala:428)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anon$1.getNext(JdbcUtils.scala:358)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anon$1.getNext(JdbcUtils.scala:340)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.NumberFormatException: For input string: \"my_table.health_center_site_fact_identification_number\"\n\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n\tat java.lang.Integer.parseInt(Integer.java:580)\n\tat java.lang.Integer.parseInt(Integer.java:615)\n\tat org.apache.hive.jdbc.HiveBaseResultSet.getInt(HiveBaseResultSet.java:347)\n\t... 27 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.sql.SQLException: Cannot convert column 1 to integerjava.lang.NumberFormatException: For input string: \"my_table.health_center_site_fact_identification_number\"\n\tat org.apache.hive.jdbc.HiveBaseResultSet.getInt(HiveBaseResultSet.java:352)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$makeGetter$7(JdbcUtils.scala:429)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$makeGetter$7$adapted(JdbcUtils.scala:428)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anon$1.getNext(JdbcUtils.scala:358)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anon$1.getNext(JdbcUtils.scala:340)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.NumberFormatException: For input string: \"my_table.health_center_site_fact_identification_number\"\n\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n\tat java.lang.Integer.parseInt(Integer.java:580)\n\tat java.lang.Integer.parseInt(Integer.java:615)\n\tat org.apache.hive.jdbc.HiveBaseResultSet.getInt(HiveBaseResultSet.java:347)\n\t... 27 more\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['SPARK_HOME'] = '/usr/local/spark'\n",
    "\n",
    "import findspark\n",
    "findspark.init(\"/usr/local/spark\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Path to the Hive JDBC Connector JAR file\n",
    "hive_connector_jar = \"/usr/local/spark/jars/hive-jdbc-2.3.9.jar\"\n",
    "\n",
    "# Initialize Spark session with the Hive JDBC Connector and authentication settings\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .appName(\"HiveIntegration\") \\\n",
    "    .config(\"spark.jars\", hive_connector_jar) \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# JDBC URL for Hive\n",
    "jdbc_url = \"jdbc:hive2://localhost:10000/health_centres\"  # Replace with your Hive server URL and databas\n",
    "\n",
    "# Table name in Hive\n",
    "table_name = \"my_table\"\n",
    "\n",
    "# Load data from Hive\n",
    "df_hive = spark.read.jdbc(jdbc_url, table_name)\n",
    "\n",
    "# Show the schema and some sample data\n",
    "df_hive.printSchema()\n",
    "df_hive.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2e2e43-46bf-4625-a9a8-4d016aef97d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SQL Server Connection for Datafiniti Fast Food Restaurants\n",
    "\n",
    "import os\n",
    "os.environ['SPARK_HOME'] = '/usr/local/spark'\n",
    "\n",
    "import findspark\n",
    "findspark.init(\"/usr/local/spark\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Path to the SQL Server JDBC Connector JAR file\n",
    "sql_server_connector_jar = \"/usr/local/spark/jars/sqljdbc42.jar\"\n",
    "\n",
    "# Initialize Spark session with the SQL Server JDBC Connector\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .appName(\"SQLServerIntegration\") \\\n",
    "    .config(\"spark.jars\", sql_server_connector_jar) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# JDBC URL for SQL Server\n",
    "jdbc_url = \"jdbc:sqlserver://localhost:1433;databaseName=fast_food\" \n",
    "\n",
    "# Connection properties\n",
    "connection_properties = {\n",
    "    \"user\": \"AzureUser\",\n",
    "    \"password\": \"MyP@55word\",\n",
    "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}\n",
    "\n",
    "# Table name in SQL Server\n",
    "table_name = \"fast_food_table\"\n",
    "\n",
    "# Load data from SQL Server\n",
    "df_sql_server = spark.read.jdbc(url=jdbc_url, table=table_name, properties=connection_properties)\n",
    "\n",
    "# Show the schema and some sample data\n",
    "df_sql_server.printSchema()\n",
    "df_sql_server.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b71408-52f5-4bc8-92df-d6417821ce72",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SQL Server Connection for Smoke Alarm\n",
    "\n",
    "import os\n",
    "os.environ['SPARK_HOME'] = '/usr/local/spark'\n",
    "\n",
    "import findspark\n",
    "findspark.init(\"/usr/local/spark\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Path to the SQL Server JDBC Connector JAR file\n",
    "sql_server_connector_jar = \"/usr/local/spark/jars/sqljdbc42.jar\"\n",
    "\n",
    "# Initialize Spark session with the SQL Server JDBC Connector\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .appName(\"SQLServerIntegration\") \\\n",
    "    .config(\"spark.jars\", sql_server_connector_jar) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# JDBC URL for SQL Server\n",
    "jdbc_url = \"jdbc:sqlserver://localhost:1433;databaseName=smoke_alarm\" \n",
    "\n",
    "# Connection properties\n",
    "connection_properties = {\n",
    "    \"user\": \"AzureUser\",\n",
    "    \"password\": \"MyP@55word\",\n",
    "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}\n",
    "\n",
    "# Table name in SQL Server\n",
    "table_name = \"smoke_alarm_table\"\n",
    "\n",
    "# Load data from SQL Server\n",
    "df_sql_server = spark.read.jdbc(url=jdbc_url, table=table_name, properties=connection_properties)\n",
    "\n",
    "# Show the schema and some sample data\n",
    "df_sql_server.printSchema()\n",
    "df_sql_server.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adc45b8-093a-4a24-8ca7-8831b3447d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataPipelineProject\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5e79b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "\n",
    "# Replace <password> with your actual password\n",
    "uri = \"mongodb+srv://viditsuri:LNL8QwaQeJZBPnlC@cluster0.k0d3iah.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
    "\n",
    "# Create a new client and connect to the server\n",
    "client = MongoClient(uri, server_api=ServerApi('1'))\n",
    "\n",
    "# Send a ping to confirm a successful connection\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d52978",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MongoDB connection for Economic Incentives\n",
    "\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "\n",
    "# Replace <password> with your actual password\n",
    "uri = \"mongodb+srv://viditsuri:LNL8QwaQeJZBPnlC@cluster0.k0d3iah.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
    "\n",
    "# Create a new client and connect to the server\n",
    "client = MongoClient(uri, server_api=ServerApi('1'))\n",
    "\n",
    "# Send a ping to confirm a successful connection\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# List all databases\n",
    "print(\"Databases:\")\n",
    "databases = client.list_database_names()\n",
    "for db in databases:\n",
    "    print(f\"- {db}\")\n",
    "\n",
    "# Access a specific database and collection\n",
    "db = client['Economic_incentives_json']\n",
    "collection = db['Economic_incentives']\n",
    "\n",
    "# Insert a document\n",
    "doc = {\"name\": \"Test Document\", \"value\": 123}\n",
    "insert_result = collection.insert_one(doc)\n",
    "print(f\"Inserted document ID: {insert_result.inserted_id}\")\n",
    "\n",
    "# Read documents\n",
    "print(\"Documents in collection:\")\n",
    "for document in collection.find():\n",
    "    print(document)\n",
    "\n",
    "# Update a document\n",
    "update_result = collection.update_one({\"name\": \"Test Document\"}, {\"$set\": {\"value\": 456}})\n",
    "print(f\"Matched {update_result.matched_count} document(s) and modified {update_result.modified_count} document(s).\")\n",
    "\n",
    "# Delete a document\n",
    "delete_result = collection.delete_one({\"name\": \"Test Document\"})\n",
    "print(f\"Deleted {delete_result.deleted_count} document(s).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aa0da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join all dataframes on the column 'postal_code'\n",
    "df_combined = df_mariadb \\\n",
    "    .join(df_hive, \"postal_code\") \\\n",
    "    .join(df_sql_server, \"postal_code\") \\\n",
    "    .join(df_sql_server2, \"postal_code\") \\\n",
    "    .join(df_mongodb, \"postal_code\")\n",
    "\n",
    "# Show the schema and some sample data of the combined dataframe\n",
    "df_combined.printSchema()\n",
    "df_combined.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
